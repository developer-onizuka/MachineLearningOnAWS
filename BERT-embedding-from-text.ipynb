{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f591733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f882eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f189e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting jupyterlab-widgets~=3.0.9\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.11.0)\n",
      "Collecting comm>=0.1.3\n",
      "  Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Collecting ydata-profiling\n",
      "  Downloading ydata_profiling-4.6.3-py2.py3-none-any.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.6/357.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.7.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (0.12.2)\n",
      "Collecting multimethod<2,>=1.4\n",
      "  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: scipy<1.12,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (1.6.3)\n",
      "Requirement already satisfied: numba<0.59.0,>=0.56.0 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (0.56.4+1.g9a03de713)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (3.1.2)\n",
      "Collecting visions[type_image_path]==0.7.5\n",
      "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<3.9,>=3.2 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (3.7.0)\n",
      "Collecting wordcloud>=1.9.1\n",
      "  Downloading wordcloud-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (520 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.1/520.1 kB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting statsmodels<1,>=0.13.2\n",
      "  Downloading statsmodels-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hCollecting pydantic>=2\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting imagehash==4.3.1\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typeguard<5,>=4.1.2\n",
      "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (1.5.2)\n",
      "Requirement already satisfied: numpy<1.26,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (1.22.2)\n",
      "Collecting htmlmin==0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (4.64.1)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (2.28.2)\n",
      "Collecting dacite>=1.8\n",
      "  Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Collecting phik<0.13,>=0.11.1\n",
      "  Downloading phik-0.12.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.5/679.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from ydata-profiling->pandas-profiling) (6.0)\n",
      "Collecting PyWavelets\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (9.4.0)\n",
      "Collecting tangled-up-in-unicode>=0.0.4\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.8/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (2.8.8)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.8/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (22.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (2.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (23.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba<0.59.0,>=0.56.0->ydata-profiling->pandas-profiling) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba<0.59.0,>=0.56.0->ydata-profiling->pandas-profiling) (6.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling->pandas-profiling) (2022.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting pydantic-core==2.14.6\n",
      "  Downloading pydantic_core-2.14.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (2022.12.7)\n",
      "Collecting patsy>=0.5.4\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba<0.59.0,>=0.56.0->ydata-profiling->pandas-profiling) (3.14.0)\n",
      "Building wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27095 sha256=499deedb28c7f95f3257188544ab6c00840724d0110c3c4715a7aaa04108ae8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/14/6e/4be5bfeeb027f4939a01764b48edd5996acf574b0913fe5243\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, widgetsnbextension, typing-extensions, tangled-up-in-unicode, PyWavelets, patsy, multimethod, jupyterlab-widgets, dacite, comm, typeguard, pydantic-core, imagehash, annotated-types, wordcloud, visions, statsmodels, pydantic, phik, ipywidgets, ydata-profiling, pandas-profiling\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "Successfully installed PyWavelets-1.4.1 annotated-types-0.6.0 comm-0.2.1 dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 multimethod-1.10 pandas-profiling-3.6.6 patsy-0.5.6 phik-0.12.3 pydantic-2.5.3 pydantic-core-2.14.6 statsmodels-0.14.1 tangled-up-in-unicode-0.2.0 typeguard-4.1.5 typing-extensions-4.9.0 visions-0.7.5 widgetsnbextension-4.0.9 wordcloud-1.9.3 ydata-profiling-4.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c394965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843cbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 04:41:07.956089: I tensorflow/stream_executor/platform/default/dso_loader.cc:50] Successfully opened dynamic library libcudart.so.12\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f50bbcf0c341ceb8406a0867900e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04b02ec2e294146bd4c580c4a02ed00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070ff437d672470681b3d12b64f34a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6649439be144988f759116912fc1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "max_seq_length = 64\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "REVIEW_BODY_COLUMN = \"review_body\"\n",
    "REVIEW_ID_COLUMN = \"review_id\"\n",
    "\n",
    "LABEL_COLUMN = \"star_rating\"\n",
    "LABEL_VALUES = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f87c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbfefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2714802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e32241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"BERT特徴量ベクトル\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a460965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(object):\n",
    "    \"\"\"シーケンス分類で用いるトレーニング/テストの単一の入力\"\"\"\n",
    "\n",
    "    def __init__(self, text, review_id, date, label=None):\n",
    "        \"\"\"入力のコンストラクタ\n",
    "        Args:\n",
    "          text: 文字列。トークン化されていない一つ目のシーケンスのテキスト。\n",
    "            単一シーケンスのタスクではこのシーケンスのみを指定する。\n",
    "          label: (オプショナル) 文字列。サンプルのラベル。トレーニングや検証用のサンプルでは指定する。\n",
    "            テスト用のサンプルでは指定しない。\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f9ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(the_input, max_seq_length):\n",
    "    # まず、BERTが学習したデータ形式と合うようにデータを前処理する。\n",
    "    # 1. テキストを小文字にする（BERT lowercaseモデルを用いる場合）\n",
    "    # 2. トークン化する（例、\"sally says hi\" -> [\"sally\", \"says\", \"hi\"]）\n",
    "    # 3. 単語をWordPieceに分割（例、\"calling\" -> [\"call\", \"##ing\"]）\n",
    "    #\n",
    "    # この辺りの処理はTransformersライブラリのトークナイザーがまかなってくれます。\n",
    "\n",
    "    tokens = tokenizer.tokenize(the_input.text)\n",
    "    tokens.insert(0, '[CLS]')\n",
    "    tokens.append('[SEP]')\n",
    "    # print(\"**{} tokens**\\n{}\\n\".format(len(tokens), tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(\n",
    "        the_input.text,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # 事前学習済みBERTの語彙ID。トークンを表す。（トークン数が `max_seq_length` 未満であれば0をパディングする）\n",
    "    input_ids = encode_plus_tokens[\"input_ids\"]\n",
    "\n",
    "    # BERTがどのトークンに注目するかを0/1で指定。`input_ids` のパディング部分のベクトル要素には0を割り当てる。\n",
    "    input_mask = encode_plus_tokens[\"attention_mask\"]\n",
    "\n",
    "    # テキスト分類のような単一シーケンスのタスクではセグメントIDは常に0とする。質問回答や次文予測のような2シーケンスタスクの場合は1を割り当てる。\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # それぞれのトレーニングデータの行のラベル（`star_rating` 1〜5）\n",
    "    label_id = label_map[the_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        review_id=the_input.review_id,\n",
    "        date=the_input.date,\n",
    "        label=the_input.label,\n",
    "    )\n",
    "\n",
    "    # print(\"**{} input_ids**\\n{}\\n\".format(len(features.input_ids), features.input_ids))\n",
    "    # print(\"**{} input_mask**\\n{}\\n\".format(len(features.input_mask), features.input_mask))\n",
    "    # print(\"**{} segment_ids**\\n{}\\n\".format(len(features.segment_ids), features.segment_ids))\n",
    "    # print(\"**label_id**\\n{}\\n\".format(features.label_id))\n",
    "    # print(\"**review_id**\\n{}\\n\".format(features.review_id))\n",
    "    # print(\"**date**\\n{}\\n\".format(features.date))\n",
    "    # print(\"**label**\\n{}\\n\".format(features.label))\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac05b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs_to_tfrecord(inputs, output_file, max_seq_length):\n",
    "    # データをBERTが理解できるフォーマットに変換する\n",
    "    records = []\n",
    "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, the_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing input {} of {}\\n\".format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(the_input, max_seq_length)\n",
    "\n",
    "        all_features = collections.OrderedDict()\n",
    "\n",
    "        # input_ids、input_mask、segment_ids、label_idsを含んだTFRecordを作成\n",
    "        all_features[\"input_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features[\"input_mask\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features[\"segment_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features[\"label_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "        tf_record_writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Feature Storeに格納する、すべての特徴量を含んだレコードを作成\n",
    "        records.append(\n",
    "            {\n",
    "                \"input_ids\": features.input_ids,\n",
    "                \"input_mask\": features.input_mask,\n",
    "                \"segment_ids\": features.segment_ids,\n",
    "                \"label_id\": features.label_id,\n",
    "                \"review_id\": the_input.review_id,\n",
    "                \"date\": the_input.date,\n",
    "                \"label\": features.label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    tf_record_writer.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f3543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a96b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-08T04:42:11Z\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "# timestamp = datetime.now().replace(microsecond=0).isoformat()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903d41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        5,\n",
    "        \"ABCD12345\",\n",
    "        \"\"\"I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\"\"\",\n",
    "    ],\n",
    "    [\n",
    "        3,\n",
    "        \"EFGH12345\",\n",
    "        \"\"\"The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\"\"\",\n",
    "    ],\n",
    "    [\n",
    "        1,\n",
    "        \"IJKL2345\",\n",
    "        \"\"\"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"\"\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"star_rating\", \"review_id\", \"review_body\"])\n",
    "\n",
    "# Input クラスを使用して、データからサンプルを作成する。\n",
    "inputs = df.apply(\n",
    "    lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56eae43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating  review_id                                        review_body\n",
       "0            5  ABCD12345  I needed an \"antivirus\" application and know t...\n",
       "1            3  EFGH12345  The problem with ElephantDrive is that it requ...\n",
       "2            1   IJKL2345  Terrible, none of my codes worked, and I can't..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3649f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <__main__.Input object at 0x7f124b78af70>\n",
       "1    <__main__.Input object at 0x7f124b78a970>\n",
       "2    <__main__.Input object at 0x7f124b78aac0>\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1092e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-08T04:42:11Z\n"
     ]
    }
   ],
   "source": [
    "# date が Feature Store の仕様に合わせて ISO-8601 になっていることを確認\n",
    "print(inputs[0].date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40100b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1330704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50d807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c41204a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/mnt/amazon_reviews_2015.snappy.parquet\",columns=[\"star_rating\",\"review_id\",\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e04a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.tail(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "621b7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7661d250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41705631</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R3H2ZM2GXRB8HD'</td>\n",
       "      <td>b\"These shoes are super comfortable for how cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705632</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R24R16PK9ZDZ7S'</td>\n",
       "      <td>b'Very comfortable&lt;br /&gt;durable material&lt;br /&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705633</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R2A53F7B9BYLN0'</td>\n",
       "      <td>b'My granddaughter has got a lot of use out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705634</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R3C55B1AOM8WU'</td>\n",
       "      <td>b'I ordered 1/2 size larger than I normally we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705635</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R2SF47JC5XG00T'</td>\n",
       "      <td>b'This is the most beautiful wallet I have eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905626</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R2341YPSNIJ3NB'</td>\n",
       "      <td>b\"I got this case for my violin, as my old one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905627</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R34HOANGHY4878'</td>\n",
       "      <td>b'just as excpected'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905628</th>\n",
       "      <td>4</td>\n",
       "      <td>b'R3APW14Y9V4QOP'</td>\n",
       "      <td>b\"It has ten really cool sounds, the beat step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905629</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R18BIAZS3JP0MI'</td>\n",
       "      <td>b'very clear sound thank you amazon i recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905630</th>\n",
       "      <td>4</td>\n",
       "      <td>b'R22EKJX8BS3YYJ'</td>\n",
       "      <td>b'Very nice rebound and sound.'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          star_rating          review_id  \\\n",
       "41705631            5  b'R3H2ZM2GXRB8HD'   \n",
       "41705632            5  b'R24R16PK9ZDZ7S'   \n",
       "41705633            5  b'R2A53F7B9BYLN0'   \n",
       "41705634            5   b'R3C55B1AOM8WU'   \n",
       "41705635            5  b'R2SF47JC5XG00T'   \n",
       "...               ...                ...   \n",
       "41905626            5  b'R2341YPSNIJ3NB'   \n",
       "41905627            5  b'R34HOANGHY4878'   \n",
       "41905628            4  b'R3APW14Y9V4QOP'   \n",
       "41905629            5  b'R18BIAZS3JP0MI'   \n",
       "41905630            4  b'R22EKJX8BS3YYJ'   \n",
       "\n",
       "                                                review_body  \n",
       "41705631  b\"These shoes are super comfortable for how cu...  \n",
       "41705632  b'Very comfortable<br />durable material<br />...  \n",
       "41705633  b'My granddaughter has got a lot of use out of...  \n",
       "41705634  b'I ordered 1/2 size larger than I normally we...  \n",
       "41705635  b'This is the most beautiful wallet I have eve...  \n",
       "...                                                     ...  \n",
       "41905626  b\"I got this case for my violin, as my old one...  \n",
       "41905627                               b'just as excpected'  \n",
       "41905628  b\"It has ten really cool sounds, the beat step...  \n",
       "41905629  b'very clear sound thank you amazon i recommen...  \n",
       "41905630                    b'Very nice rebound and sound.'  \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63c7f502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R2C20GSMIOZYVP'</td>\n",
       "      <td>b'I have made multiple purchases of this prosc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>b'RPI30SPP1J9U9'</td>\n",
       "      <td>b\"I am not sure if it's a product or storage p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b'RKYY2ZQGUV06L'</td>\n",
       "      <td>b\"I was  hoping this had a stronger taste than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>b'RKYYAEA9G3CD4'</td>\n",
       "      <td>b'Awesome Tea!'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>b'R17ZQPU555KVR6'</td>\n",
       "      <td>b\"This tasty spread tastes just like a melted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R1GLNC58UW0GWH'</td>\n",
       "      <td>b'Very cute and fashionable!!'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>4</td>\n",
       "      <td>b'R3J6QEWMIDM9F7'</td>\n",
       "      <td>b\"Takes longer to fully dry other than that's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R2N13SMS1UIYVH'</td>\n",
       "      <td>b'The wife likes it so I like it.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>4</td>\n",
       "      <td>b'RJJ4WC7L6VU7Z'</td>\n",
       "      <td>b'Everyday liking my case at work'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>5</td>\n",
       "      <td>b'R25KGP41DPK52V'</td>\n",
       "      <td>b'I have sehborhic dermatitis and this is the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating          review_id  \\\n",
       "0                 5  b'R2C20GSMIOZYVP'   \n",
       "1                 3   b'RPI30SPP1J9U9'   \n",
       "2                 1   b'RKYY2ZQGUV06L'   \n",
       "3                 5   b'RKYYAEA9G3CD4'   \n",
       "4                 4  b'R17ZQPU555KVR6'   \n",
       "...             ...                ...   \n",
       "999995            5  b'R1GLNC58UW0GWH'   \n",
       "999996            4  b'R3J6QEWMIDM9F7'   \n",
       "999997            5  b'R2N13SMS1UIYVH'   \n",
       "999998            4   b'RJJ4WC7L6VU7Z'   \n",
       "999999            5  b'R25KGP41DPK52V'   \n",
       "\n",
       "                                              review_body  \n",
       "0       b'I have made multiple purchases of this prosc...  \n",
       "1       b\"I am not sure if it's a product or storage p...  \n",
       "2       b\"I was  hoping this had a stronger taste than...  \n",
       "3                                         b'Awesome Tea!'  \n",
       "4       b\"This tasty spread tastes just like a melted ...  \n",
       "...                                                   ...  \n",
       "999995                     b'Very cute and fashionable!!'  \n",
       "999996  b\"Takes longer to fully dry other than that's ...  \n",
       "999997                 b'The wife likes it so I like it.'  \n",
       "999998                 b'Everyday liking my case at work'  \n",
       "999999  b'I have sehborhic dermatitis and this is the ...  \n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13d608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_id'] = df['review_id'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62dbd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'] = df['review_body'].str.decode(\"utf-8\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02ab5532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>R2C20GSMIOZYVP</td>\n",
       "      <td>I have made multiple purchases of this prosciu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>RPI30SPP1J9U9</td>\n",
       "      <td>I am not sure if it's a product or storage pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RKYY2ZQGUV06L</td>\n",
       "      <td>I was  hoping this had a stronger taste than r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>RKYYAEA9G3CD4</td>\n",
       "      <td>Awesome Tea!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>R17ZQPU555KVR6</td>\n",
       "      <td>This tasty spread tastes just like a melted Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>5</td>\n",
       "      <td>R1GLNC58UW0GWH</td>\n",
       "      <td>Very cute and fashionable!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>4</td>\n",
       "      <td>R3J6QEWMIDM9F7</td>\n",
       "      <td>Takes longer to fully dry other than that's it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>5</td>\n",
       "      <td>R2N13SMS1UIYVH</td>\n",
       "      <td>The wife likes it so I like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>4</td>\n",
       "      <td>RJJ4WC7L6VU7Z</td>\n",
       "      <td>Everyday liking my case at work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>5</td>\n",
       "      <td>R25KGP41DPK52V</td>\n",
       "      <td>I have sehborhic dermatitis and this is the on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating       review_id  \\\n",
       "0                 5  R2C20GSMIOZYVP   \n",
       "1                 3   RPI30SPP1J9U9   \n",
       "2                 1   RKYY2ZQGUV06L   \n",
       "3                 5   RKYYAEA9G3CD4   \n",
       "4                 4  R17ZQPU555KVR6   \n",
       "...             ...             ...   \n",
       "999995            5  R1GLNC58UW0GWH   \n",
       "999996            4  R3J6QEWMIDM9F7   \n",
       "999997            5  R2N13SMS1UIYVH   \n",
       "999998            4   RJJ4WC7L6VU7Z   \n",
       "999999            5  R25KGP41DPK52V   \n",
       "\n",
       "                                              review_body  \n",
       "0       I have made multiple purchases of this prosciu...  \n",
       "1       I am not sure if it's a product or storage pro...  \n",
       "2       I was  hoping this had a stronger taste than r...  \n",
       "3                                            Awesome Tea!  \n",
       "4       This tasty spread tastes just like a melted Re...  \n",
       "...                                                   ...  \n",
       "999995                        Very cute and fashionable!!  \n",
       "999996  Takes longer to fully dry other than that's it...  \n",
       "999997                    The wife likes it so I like it.  \n",
       "999998                    Everyday liking my case at work  \n",
       "999999  I have sehborhic dermatitis and this is the on...  \n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5d22035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['review_id'] = df2['review_id'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7ae7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['review_body'] = df2['review_body'].str.decode(\"utf-8\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad389a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41705631</th>\n",
       "      <td>5</td>\n",
       "      <td>R3H2ZM2GXRB8HD</td>\n",
       "      <td>These shoes are super comfortable for how cute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705632</th>\n",
       "      <td>5</td>\n",
       "      <td>R24R16PK9ZDZ7S</td>\n",
       "      <td>Very comfortable&lt;br /&gt;durable material&lt;br /&gt;it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705633</th>\n",
       "      <td>5</td>\n",
       "      <td>R2A53F7B9BYLN0</td>\n",
       "      <td>My granddaughter has got a lot of use out of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705634</th>\n",
       "      <td>5</td>\n",
       "      <td>R3C55B1AOM8WU</td>\n",
       "      <td>I ordered 1/2 size larger than I normally wear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41705635</th>\n",
       "      <td>5</td>\n",
       "      <td>R2SF47JC5XG00T</td>\n",
       "      <td>This is the most beautiful wallet I have ever ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905626</th>\n",
       "      <td>5</td>\n",
       "      <td>R2341YPSNIJ3NB</td>\n",
       "      <td>I got this case for my violin, as my old one w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905627</th>\n",
       "      <td>5</td>\n",
       "      <td>R34HOANGHY4878</td>\n",
       "      <td>just as excpected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905628</th>\n",
       "      <td>4</td>\n",
       "      <td>R3APW14Y9V4QOP</td>\n",
       "      <td>It has ten really cool sounds, the beat step w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905629</th>\n",
       "      <td>5</td>\n",
       "      <td>R18BIAZS3JP0MI</td>\n",
       "      <td>very clear sound thank you amazon i recommende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905630</th>\n",
       "      <td>4</td>\n",
       "      <td>R22EKJX8BS3YYJ</td>\n",
       "      <td>Very nice rebound and sound.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          star_rating       review_id  \\\n",
       "41705631            5  R3H2ZM2GXRB8HD   \n",
       "41705632            5  R24R16PK9ZDZ7S   \n",
       "41705633            5  R2A53F7B9BYLN0   \n",
       "41705634            5   R3C55B1AOM8WU   \n",
       "41705635            5  R2SF47JC5XG00T   \n",
       "...               ...             ...   \n",
       "41905626            5  R2341YPSNIJ3NB   \n",
       "41905627            5  R34HOANGHY4878   \n",
       "41905628            4  R3APW14Y9V4QOP   \n",
       "41905629            5  R18BIAZS3JP0MI   \n",
       "41905630            4  R22EKJX8BS3YYJ   \n",
       "\n",
       "                                                review_body  \n",
       "41705631  These shoes are super comfortable for how cute...  \n",
       "41705632  Very comfortable<br />durable material<br />it...  \n",
       "41705633  My granddaughter has got a lot of use out of t...  \n",
       "41705634  I ordered 1/2 size larger than I normally wear...  \n",
       "41705635  This is the most beautiful wallet I have ever ...  \n",
       "...                                                     ...  \n",
       "41905626  I got this case for my violin, as my old one w...  \n",
       "41905627                                  just as excpected  \n",
       "41905628  It has ten really cool sounds, the beat step w...  \n",
       "41905629  very clear sound thank you amazon i recommende...  \n",
       "41905630                       Very nice rebound and sound.  \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f15b84a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df Memory Usage: 0.34999158699065447 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"df Memory Usage: {df.memory_usage(deep=True).sum() / 1024**3} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7661cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51a983f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input クラスを使用して、データからサンプルを作成する。\n",
    "inputs = df.apply(\n",
    "    lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd1179aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input クラスを使用して、データからサンプルを作成する。\n",
    "inputs2 = df2.apply(\n",
    "    lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bdf40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandarallel import pandarallel\n",
    "# import os\n",
    "# os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'\n",
    "# !pip3 install --no-cache-dir accelerate\n",
    "# pandarallel.initialize(nb_workers=2, progress_bar=True, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def func(x):\n",
    "#    return lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp)\n",
    "\n",
    "# inputs = df.parallel_apply(\n",
    "#     lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp),\n",
    "#     axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1f93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f049d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         <__main__.Input object at 0x7f0e68ffcc70>\n",
       "1         <__main__.Input object at 0x7f0e68ffcb80>\n",
       "2         <__main__.Input object at 0x7f0e68ffc730>\n",
       "3         <__main__.Input object at 0x7f0e68ffcee0>\n",
       "4         <__main__.Input object at 0x7f0e68ffce20>\n",
       "                            ...                    \n",
       "999995    <__main__.Input object at 0x7f104bed5d90>\n",
       "999996    <__main__.Input object at 0x7f104bed5df0>\n",
       "999997    <__main__.Input object at 0x7f104bed5e50>\n",
       "999998    <__main__.Input object at 0x7f104bed5eb0>\n",
       "999999    <__main__.Input object at 0x7f104bed5f10>\n",
       "Length: 1000000, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aea980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/mnt/data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40830e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = \"/mnt/data2.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db4f28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-08T04:42:11Z\n"
     ]
    }
   ],
   "source": [
    "# date が Feature Store の仕様に合わせて ISO-8601 になっていることを確認\n",
    "print(inputs[0].date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4675747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 1000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 10000 of 1000000\n",
      "\n",
      "Writing input 20000 of 1000000\n",
      "\n",
      "Writing input 30000 of 1000000\n",
      "\n",
      "Writing input 40000 of 1000000\n",
      "\n",
      "Writing input 50000 of 1000000\n",
      "\n",
      "Writing input 60000 of 1000000\n",
      "\n",
      "Writing input 70000 of 1000000\n",
      "\n",
      "Writing input 80000 of 1000000\n",
      "\n",
      "Writing input 90000 of 1000000\n",
      "\n",
      "Writing input 100000 of 1000000\n",
      "\n",
      "Writing input 110000 of 1000000\n",
      "\n",
      "Writing input 120000 of 1000000\n",
      "\n",
      "Writing input 130000 of 1000000\n",
      "\n",
      "Writing input 140000 of 1000000\n",
      "\n",
      "Writing input 150000 of 1000000\n",
      "\n",
      "Writing input 160000 of 1000000\n",
      "\n",
      "Writing input 170000 of 1000000\n",
      "\n",
      "Writing input 180000 of 1000000\n",
      "\n",
      "Writing input 190000 of 1000000\n",
      "\n",
      "Writing input 200000 of 1000000\n",
      "\n",
      "Writing input 210000 of 1000000\n",
      "\n",
      "Writing input 220000 of 1000000\n",
      "\n",
      "Writing input 230000 of 1000000\n",
      "\n",
      "Writing input 240000 of 1000000\n",
      "\n",
      "Writing input 250000 of 1000000\n",
      "\n",
      "Writing input 260000 of 1000000\n",
      "\n",
      "Writing input 270000 of 1000000\n",
      "\n",
      "Writing input 280000 of 1000000\n",
      "\n",
      "Writing input 290000 of 1000000\n",
      "\n",
      "Writing input 300000 of 1000000\n",
      "\n",
      "Writing input 310000 of 1000000\n",
      "\n",
      "Writing input 320000 of 1000000\n",
      "\n",
      "Writing input 330000 of 1000000\n",
      "\n",
      "Writing input 340000 of 1000000\n",
      "\n",
      "Writing input 350000 of 1000000\n",
      "\n",
      "Writing input 360000 of 1000000\n",
      "\n",
      "Writing input 370000 of 1000000\n",
      "\n",
      "Writing input 380000 of 1000000\n",
      "\n",
      "Writing input 390000 of 1000000\n",
      "\n",
      "Writing input 400000 of 1000000\n",
      "\n",
      "Writing input 410000 of 1000000\n",
      "\n",
      "Writing input 420000 of 1000000\n",
      "\n",
      "Writing input 430000 of 1000000\n",
      "\n",
      "Writing input 440000 of 1000000\n",
      "\n",
      "Writing input 450000 of 1000000\n",
      "\n",
      "Writing input 460000 of 1000000\n",
      "\n",
      "Writing input 470000 of 1000000\n",
      "\n",
      "Writing input 480000 of 1000000\n",
      "\n",
      "Writing input 490000 of 1000000\n",
      "\n",
      "Writing input 500000 of 1000000\n",
      "\n",
      "Writing input 510000 of 1000000\n",
      "\n",
      "Writing input 520000 of 1000000\n",
      "\n",
      "Writing input 530000 of 1000000\n",
      "\n",
      "Writing input 540000 of 1000000\n",
      "\n",
      "Writing input 550000 of 1000000\n",
      "\n",
      "Writing input 560000 of 1000000\n",
      "\n",
      "Writing input 570000 of 1000000\n",
      "\n",
      "Writing input 580000 of 1000000\n",
      "\n",
      "Writing input 590000 of 1000000\n",
      "\n",
      "Writing input 600000 of 1000000\n",
      "\n",
      "Writing input 610000 of 1000000\n",
      "\n",
      "Writing input 620000 of 1000000\n",
      "\n",
      "Writing input 630000 of 1000000\n",
      "\n",
      "Writing input 640000 of 1000000\n",
      "\n",
      "Writing input 650000 of 1000000\n",
      "\n",
      "Writing input 660000 of 1000000\n",
      "\n",
      "Writing input 670000 of 1000000\n",
      "\n",
      "Writing input 680000 of 1000000\n",
      "\n",
      "Writing input 690000 of 1000000\n",
      "\n",
      "Writing input 700000 of 1000000\n",
      "\n",
      "Writing input 710000 of 1000000\n",
      "\n",
      "Writing input 720000 of 1000000\n",
      "\n",
      "Writing input 730000 of 1000000\n",
      "\n",
      "Writing input 740000 of 1000000\n",
      "\n",
      "Writing input 750000 of 1000000\n",
      "\n",
      "Writing input 760000 of 1000000\n",
      "\n",
      "Writing input 770000 of 1000000\n",
      "\n",
      "Writing input 780000 of 1000000\n",
      "\n",
      "Writing input 790000 of 1000000\n",
      "\n",
      "Writing input 800000 of 1000000\n",
      "\n",
      "Writing input 810000 of 1000000\n",
      "\n",
      "Writing input 820000 of 1000000\n",
      "\n",
      "Writing input 830000 of 1000000\n",
      "\n",
      "Writing input 840000 of 1000000\n",
      "\n",
      "Writing input 850000 of 1000000\n",
      "\n",
      "Writing input 860000 of 1000000\n",
      "\n",
      "Writing input 870000 of 1000000\n",
      "\n",
      "Writing input 880000 of 1000000\n",
      "\n",
      "Writing input 890000 of 1000000\n",
      "\n",
      "Writing input 900000 of 1000000\n",
      "\n",
      "Writing input 910000 of 1000000\n",
      "\n",
      "Writing input 920000 of 1000000\n",
      "\n",
      "Writing input 930000 of 1000000\n",
      "\n",
      "Writing input 940000 of 1000000\n",
      "\n",
      "Writing input 950000 of 1000000\n",
      "\n",
      "Writing input 960000 of 1000000\n",
      "\n",
      "Writing input 970000 of 1000000\n",
      "\n",
      "Writing input 980000 of 1000000\n",
      "\n",
      "Writing input 990000 of 1000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4135685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 200000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 10000 of 200000\n",
      "\n",
      "Writing input 20000 of 200000\n",
      "\n",
      "Writing input 30000 of 200000\n",
      "\n",
      "Writing input 40000 of 200000\n",
      "\n",
      "Writing input 50000 of 200000\n",
      "\n",
      "Writing input 60000 of 200000\n",
      "\n",
      "Writing input 70000 of 200000\n",
      "\n",
      "Writing input 80000 of 200000\n",
      "\n",
      "Writing input 90000 of 200000\n",
      "\n",
      "Writing input 100000 of 200000\n",
      "\n",
      "Writing input 110000 of 200000\n",
      "\n",
      "Writing input 120000 of 200000\n",
      "\n",
      "Writing input 130000 of 200000\n",
      "\n",
      "Writing input 140000 of 200000\n",
      "\n",
      "Writing input 150000 of 200000\n",
      "\n",
      "Writing input 160000 of 200000\n",
      "\n",
      "Writing input 170000 of 200000\n",
      "\n",
      "Writing input 180000 of 200000\n",
      "\n",
      "Writing input 190000 of 200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records2 = transform_inputs_to_tfrecord(inputs2, output_file2, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19164a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2c5796a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "        \n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e7d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
